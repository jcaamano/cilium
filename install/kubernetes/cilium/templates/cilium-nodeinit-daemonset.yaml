{{- if .Values.nodeinit.enabled }}
kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: cilium-node-init
  namespace: {{ .Release.Namespace }}
  labels:
    app: cilium-node-init
spec:
  selector:
    matchLabels:
      app: cilium-node-init
  template:
    metadata:
      annotations:
      labels:
        app: cilium-node-init
    spec:
      tolerations:
      - operator: Exists
      hostPID: true
      hostNetwork: true
{{- if (or (and (eq .Release.Namespace "kube-system") (gt .Capabilities.KubeVersion.Minor "10")) (ge .Capabilities.KubeVersion.Minor "17") (gt .Capabilities.KubeVersion.Major "1")) }}
      priorityClassName: system-node-critical
{{- end }}
{{- if .Values.imagePullSecrets }}
      imagePullSecrets:
        {{ toYaml .Values.imagePullSecrets | indent 6 }}
{{- end }}
      containers:
        - name: node-init
          image: {{ .Values.nodeinit.image.repository }}:{{ .Values.nodeinit.image.tag }}
          imagePullPolicy: {{ .Values.nodeinit.image.pullPolicy }}
          securityContext:
            privileged: true
          env:
          - name: CHECKPOINT_PATH
            value: /tmp/node-init.cilium.io
          # STARTUP_SCRIPT is the script run on node bootstrap. Node
          # bootstrapping can be customized in this script. This script is invoked
          # using nsenter, so it runs in the host's network and mount namespace using
          # the host's userland tools!
          - name: STARTUP_SCRIPT
            value: |
              #!/bin/bash

              set -o errexit
              set -o pipefail
              set -o nounset

              mount | grep "/sys/fs/bpf type bpf" || {
                # Mount the filesystem until next reboot
                echo "Mounting BPF filesystem..."
                mount bpffs /sys/fs/bpf -t bpf

                # Configure systemd to mount after next boot
                echo "Installing BPF filesystem mount"
                cat >/tmp/sys-fs-bpf.mount <<EOF
              [Unit]
              Description=Mount BPF filesystem (Cilium)
              Documentation=http://docs.cilium.io/
              DefaultDependencies=no
              Before=local-fs.target umount.target
              After=swap.target

              [Mount]
              What=bpffs
              Where=/sys/fs/bpf
              Type=bpf
              Options=rw,nosuid,nodev,noexec,relatime,mode=700

              [Install]
              WantedBy=multi-user.target
              EOF

                if [ -d "/etc/systemd/system/" ]; then
                  mv /tmp/sys-fs-bpf.mount /etc/systemd/system/
                  echo "Installed sys-fs-bpf.mount to /etc/systemd/system/"
                elif [ -d "/lib/systemd/system/" ]; then
                  mv /tmp/sys-fs-bpf.mount /lib/systemd/system/
                  echo "Installed sys-fs-bpf.mount to /lib/systemd/system/"
                fi

                # Ensure that filesystem gets mounted on next reboot
                systemctl enable sys-fs-bpf.mount
                systemctl start sys-fs-bpf.mount
              }

              echo "Link information:"
              ip link

              echo "Routing table:"
              ip route

              echo "Addressing:"
              ip -4 a
              ip -6 a
              date > /tmp/cilium-bootstrap-time

              # AKS: If azure-vnet is installed on the node, and (still) configured in bridge mode,
              # configure it as 'transparent' to be consistent with Cilium's CNI chaining config.
              # If the azure-vnet CNI config is not removed, kubelet will execute CNI CHECK commands
              # against it every 5 seconds and write 'bridge' to its state file, causing inconsistent
              # behaviour when Pods are removed.
              if [ -f /etc/cni/net.d/10-azure.conflist ]; then
                echo "Ensuring azure-vnet is configured in 'transparent' mode..."
                sed -i 's/"mode":\s*"bridge"/"mode":"transparent"/g' /etc/cni/net.d/10-azure.conflist
              fi

{{- if .Values.azure.enabled }}
              # The azure0 interface being present means the node was booted with azure-vnet configured
              # in bridge mode. This means there might be ebtables rules and neight entries interfering
              # with pod connectivity if we deploy with Azure IPAM.
              if ip l show dev azure0 >/dev/null 2>&1; then

                # In Azure IPAM mode, also remove the azure-vnet state file, otherwise ebtables rules get
                # restored by the azure-vnet CNI plugin on every CNI CHECK, which can cause connectivity
                # issues in Cilium-managed Pods. Since azure-vnet is no longer called on scheduling events,
                # this file can be removed.
                rm -f /var/run/azure-vnet.json

                # This breaks connectivity for existing workload Pods when Cilium is scheduled, but we need
                # to flush these to prevent Cilium-managed Pod IPs conflicting with Pod IPs previously allocated
                # by azure-vnet. These ebtables DNAT rules contain fixed MACs that are no longer bound on the node,
                # causing packets for these Pods to be redirected back out to the gateway, where they are dropped.
                echo 'Flushing ebtables pre/postrouting rules in nat table.. (disconnecting non-Cilium Pods!)'
                ebtables -t nat -F PREROUTING || true
                ebtables -t nat -F POSTROUTING || true

                # ip-masq-agent periodically injects PERM neigh entries towards the gateway
                # for all other k8s nodes in the cluster. These are safe to flush, as ARP can
                # resolve these nodes as usual. PERM entries will be automatically restored later.
                echo 'Deleting all permanent neighbour entries on azure0...'
                ip neigh show dev azure0 nud permanent | cut -d' ' -f1 | xargs -r -n1 ip neigh del dev azure0 to || true
              fi
{{- end }}
              echo "Node initialization complete"
{{- end }}
